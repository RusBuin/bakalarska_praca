{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jUCTvLcNGCjSIOQFautM1stSQdVSGSeO",
      "authorship_tag": "ABX9TyNuRyhcLmQHcxNgkPVLOa0y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xBWz7gJZqpO",
        "outputId": "d14dc7ff-8ff7-4428-a837-8be0228f8361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Načítanie dát\n",
        "dawid = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Odstránenie prvého stĺpca\n",
        "dawid = dawid.drop(dawid.columns[0], axis=1)\n",
        "\n",
        "# Prekódovanie tried\n",
        "\n",
        "dawid.loc[dawid['class'] == 0, 'class'] = 1\n",
        "dawid.loc[dawid['class'] == 2, 'class'] = 0\n",
        "\n",
        "# Odstránenie nepotrebných stĺpcov\n",
        "dawid = dawid.drop(columns=['count', 'hate_speech', 'offensive_language', 'neither'])\n",
        "\n",
        "print(dawid.head(100))\n",
        "\n",
        "# Extrakcia textov a tried do samostatných premenných\n",
        "texts = dawid['tweet']\n",
        "labels = dawid['class']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-McvpEibKWn",
        "outputId": "386d9524-9b0c-45f6-c7a5-27c9265a500d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    class                                              tweet\n",
            "0       0  !!! RT @mayasolovely: As a woman you shouldn't...\n",
            "1       1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
            "2       1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
            "3       1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
            "4       1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
            "..    ...                                                ...\n",
            "95      1  \"@CauseWereGuys: Going back to school sucks mo...\n",
            "96      1  \"@CauseWereGuys: On my way to fuck yo bitch ht...\n",
            "97      1  \"@CeleyNichole: @white_thunduh how come you ne...\n",
            "98      1  \"@ChadMFVerbeck: If Richnow doesn't show up wi...\n",
            "99      1  \"@ChandlerParsons: How bout them Cowboys!!!!\" ...\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X obsahuje texty komentárov a y - ich triedy\n",
        "\n",
        "# Rozdelenie na trénovaciu a testovaciu sadu (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vytvorenie train dat\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "train_data.to_csv('trainNO.csv', index=False)\n",
        "\n",
        "# Rozdelenie train dát na toxické a netoxické komentáre\n",
        "toxic_comments = train_data[train_data['class'] == 1]\n",
        "non_toxic_comments = train_data[train_data['class'] == 0]\n",
        "\n",
        "# Zníženie počtu toxických komentárov na rovnaké množstvo ako netoxické\n",
        "toxic_comments_downsampled = resample(toxic_comments,\n",
        "                                     replace=False,  # Nezameníme prvky\n",
        "                                     n_samples=len(non_toxic_comments),  # Nastavoovanie počtu na rovnaké množstvo ako netoxické komentáre\n",
        "                                     random_state=42)\n",
        "\n",
        "# Spojenie zníženého množstva toxických komentárov s netoxickými\n",
        "balanced_train_data = pd.concat([toxic_comments_downsampled, non_toxic_comments])\n",
        "\n",
        "# Premiešanie dát\n",
        "balanced_train_data = balanced_train_data.sample(frac=1, random_state=42)\n",
        "\n",
        "\n",
        "X_train_balanced = balanced_train_data.drop('class', axis=1)\n",
        "y_train_balanced = balanced_train_data['class']\n",
        "\n",
        "# Spojenie do jedného datasetu pre tréning a test\n",
        "train = pd.concat([X_train_balanced, y_train_balanced], axis=1)\n",
        "test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# Prevedenie tried na one-hot encoding\n",
        "one_hot_labels = to_categorical(labels, num_classes=2)"
      ],
      "metadata": {
        "id": "cRzciBNDbXTE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "-xFhlDusc0jk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}